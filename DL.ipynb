{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEhcsaMVKKbV"
      },
      "outputs": [],
      "source": [
        "// autoe\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(train_images,__),(test_images,_)=mnist.load_data()\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(test_images[0].reshape(28,28))\n",
        "plt.gray()\n",
        "\n",
        "train_images=train_images.astype('float32')/255.\n",
        "test_images=test_images.astype('float32')/255.\n",
        "\n",
        "train_images=train_images.reshape((len(train_images),np.prod(train_images.shape[1:])))\n",
        "test_images=test_images.reshape((len(test_images),np.prod(test_images.shape[1:])))\n",
        "print(train_images.shape)\n",
        "print(train_images.shape)\n",
        "\n",
        "from keras.layers import Input,Dense\n",
        "from keras.models import Model\n",
        "encoding_dim=32\n",
        "input_layer=Input(shape=(784,))\n",
        "encoder_layer1=Dense(encoding_dim,activation='relu')(input_layer)\n",
        "decoder_layer1=Dense(784,activation='sigmoid')(encoder_layer1)\n",
        "autoencoder=Model(input_layer,decoder_layer1)\n",
        "autoencoder.summary()\n",
        "\n",
        "encoder=Model(input_layer,encoder_layer1)\n",
        "\n",
        "encoded_input=Input(shape=(encoding_dim,))\n",
        "decoder_layer=autoencoder.layers[-1]\n",
        "decoder=Model(encoded_input,decoder_layer(encoded_input))\n",
        "\n",
        "autoencoder.compile(optimizer='adam',loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(train_images,train_images,\n",
        "epochs=60,\n",
        "batch_size=256,\n",
        "shuffle=True,\n",
        "validation_data=(test_images,test_images))\n",
        "\n",
        "encoded_imgs=encoder.predict(test_images)\n",
        "print(encoded_imgs.shape)\n",
        "\n",
        "decoded_imgs=decoder.predict(encoded_imgs)\n",
        "print(decoded_imgs.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10  # How many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(test_images[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "// backpr\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Sigmoid function\n",
        "def nlinear(x,deriv=False):\n",
        "  if(deriv==True):\n",
        "    return x*(1-x)\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# Input\n",
        "X=np.array([[1,1,0],[0,1,1],[0,0,1],[1,1,0]])\n",
        "# Output\n",
        "y=np.array([[1,0,0,1]]).T\n",
        "# Seed for random number distribution\n",
        "#np.random.seed(1)\n",
        "# Weights initialization\n",
        "synapse0=2*np.random.random((3,1))-1\n",
        "\n",
        "for i in range(1000):\n",
        "  # Forward propagation\n",
        "  layer0=X\n",
        "  layer1=nlinear(np.dot(layer0,synapse0))\n",
        "  # Error\n",
        "  layer1_error=y-layer1\n",
        "  # Multiply with error backpropagated\n",
        "  layer1_delta=layer1_error*nlinear(layer1,True)\n",
        "  # Update wts\n",
        "  synapse0+=np.dot(layer0.T,layer1_delta)\n",
        "\n",
        "print(\"Predicted Output:\")\n",
        "print(layer1)\n",
        "print(\"Actual Output:\")\n",
        "print(y)\n",
        "\n",
        "df=[y,layer1]\n",
        "df\n",
        "\n",
        "plt.plot(y,layer1)\n",
        "\n"
      ],
      "metadata": {
        "id": "GYzOmamgKgED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// cnn\n",
        "\n",
        "import tensorflow as tf\n",
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(y_train[1])\n",
        "plt.imshow(x_train[1],cmap='Greys')\n",
        "\n",
        "x_train.shape\n",
        "\n",
        "x_train=x_train.reshape(x_train.shape[0],28,28,1)\n",
        "x_test=x_test.reshape(x_test.shape[0],28,28,1)\n",
        "input_shape=(28,28,1)\n",
        "x_train=x_train.astype('float32')\n",
        "x_test=x_test.astype('float32')\n",
        "x_train/=255\n",
        "x_test/=255\n",
        "print(x_train.shape[0])\n",
        "print(x_test.shape[0])\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Dropout,Flatten,MaxPooling2D\n",
        "model=Sequential()\n",
        "model.add(Conv2D(32,kernel_size=(3,3),input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32,kernel_size=(3,3),input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation=tf.nn.relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation=tf.nn.softmax))\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x=x_train,y=y_train, epochs=10)\n",
        "\n",
        "model.evaluate(x_test,y_test)\n",
        "\n",
        "image_index=6630\n",
        "plt.imshow(x_test[image_index].reshape(28,28),cmap='Greys')\n",
        "pred=model.predict(x_test[image_index].reshape(1,28,28,1))\n",
        "print(pred.argmax())\n"
      ],
      "metadata": {
        "id": "rNiinjyfLJlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// lstm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM, Dense,Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import csv\n",
        "\n",
        "imdb = pd.read_csv('/content/drive/MyDrive/Data/IMDB Dataset.csv',engine=\"python\")\n",
        "imdb.head()\n",
        "\n",
        "imdb.sentiment.value_counts()\n",
        "\n",
        "text=imdb['review'][10]\n",
        "print(text)\n",
        "print(\"---------------------\")\n",
        "print(word_tokenize(text))\n",
        "\n",
        "corpus=[]\n",
        "for text in imdb['review']:\n",
        "  words=[word.lower() for word in word_tokenize(text)]\n",
        "  corpus.append(words)\n",
        "\n",
        "num_words=len(corpus)\n",
        "print(num_words)\n",
        "\n",
        "imdb.shape\n",
        "\n",
        "train_size=int(imdb.shape[0]*0.8)\n",
        "X_train=imdb.review[:train_size]\n",
        "y_train=imdb.sentiment[:train_size]\n",
        "\n",
        "X_test=imdb.review[train_size:]\n",
        "y_test=imdb.sentiment[train_size:]\n",
        "\n",
        "tokenizer=Tokenizer(num_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train=tokenizer.texts_to_sequences(X_train)\n",
        "X_train=pad_sequences(X_train,maxlen=128, truncating='post',padding='post')\n",
        "\n",
        "X_train[0],len(X_train[0])\n",
        "\n",
        "X_test=tokenizer.texts_to_sequences(X_test)\n",
        "X_test=pad_sequences(X_test,maxlen=128,truncating='post',padding='post')\n",
        "\n",
        "X_test[0],len(X_test[0])\n",
        "\n",
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)\n",
        "\n",
        "le=LabelEncoder()\n",
        "y_train=le.fit_transform(y_train)\n",
        "y_test=le.transform(y_test)\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=num_words,output_dim=100,input_length=128,trainable=True))\n",
        "model.add(LSTM(100,dropout=0.1,return_sequences=True))\n",
        "model.add(LSTM(100,dropout=0.1))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(X_train,y_train,epochs=5,batch_size=64,validation_data=(X_test,y_test))\n",
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "epochs=range(1,len(history.history['accuracy'])+1)\n",
        "plt.plot(epochs,history.history['loss'],'b',label='Training loss')\n",
        "plt.plot(epochs,history.history['val_loss'],'b',label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#validation_sentence=['this movie was not good at all. it had some good parts like the acting was pretty good but story was not impressive at all']\n",
        "validation_sentence=['i can watch the movie forever just because of beuty of cinematography']\n",
        "validation_sentence_tokened=tokenizer.texts_to_sequences(validation_sentence)\n",
        "val_sent_pad=pad_sequences(validation_sentence_tokened,maxlen=128,truncating='post',padding='post')\n",
        "print(validation_sentence[0])\n",
        "print(model.predict(val_sent_pad)[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "uQnO7PsWLU9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// mcculloch\n",
        "\n",
        "\n",
        "def fire(theta, sum):\n",
        "  if sum >= theta:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "input = [[1,1,1],[0,0,0],[1,1,0],[0,0,1]]\n",
        "\n",
        "for i in input:\n",
        "  sum = 0\n",
        "  for j in i:\n",
        "    sum += j\n",
        "  print(\"AND (\",i,\") = \", fire(len(i), sum))\n",
        "\n",
        "def fire(theta, sum):\n",
        "  if sum >= theta:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "input = [[1,1,1],[0,0,0],[1,1,0],[0,0,1]]\n",
        "\n",
        "for i in input:\n",
        "  sum = 0\n",
        "  for j in i:\n",
        "    sum += j\n",
        "  print(\"OR (\",i,\") = \", fire(1, sum))\n",
        "\n",
        "def fire(theta, sum):\n",
        "  if sum == theta:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "input = [1,0]\n",
        "\n",
        "for i in input:\n",
        "  print(\"NOT (\",i,\") = \", fire(0, i))\n",
        "\n"
      ],
      "metadata": {
        "id": "VFHLP6SuLa8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// perceptron\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "inputs = np.array([[0,0],[1,0]])\n",
        "weights = np.array([1,1])\n",
        "bias = -0.5\n",
        "\n",
        "def ORs(x, wt, b):\n",
        "  fx = np.dot(wt, x) + b\n",
        "  if fx >= 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "for i in inputs:\n",
        "  print(\"OR (\", i, \") = \", ORs(i, weights, bias))\n",
        "\n"
      ],
      "metadata": {
        "id": "KAo8WlhdLjYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// sgg\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "red_wine=pd.read_csv('red-wine.csv')\n",
        "df_train=red_wine.sample(frac=0.7,random_state=0)\n",
        "df_valid=red_wine.drop(df_train.index)\n",
        "display(df_train.head(4))\n",
        "\n",
        "max_=df_train.max(axis=0)\n",
        "min_=df_train.min(axis=0)\n",
        "df_train=(df_train-min_)/(max_-min_)\n",
        "df_valid=(df_valid-min_)/(max_-min_)\n",
        "\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']\n",
        "\n",
        "print(X_train.shape, X_valid.shape)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='mse',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "history_df=pd.DataFrame(history.history)\n",
        "history_df['loss'].plot()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1iCeGowkLpYU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}